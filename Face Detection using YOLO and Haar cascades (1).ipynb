{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f4a5d6",
   "metadata": {},
   "source": [
    "# Real-time face detection- Cyber Security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891e4b7",
   "metadata": {},
   "source": [
    "# By OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108778d8",
   "metadata": {},
   "source": [
    "# Introduction :- \n",
    "\n",
    "OpenCV provides various trained models for different computer vision tasks. Here are some common types of OpenCV models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50799a",
   "metadata": {},
   "source": [
    "# Some common types of OpenCV models:-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba241356",
   "metadata": {},
   "source": [
    "\n",
    "1. Haar Cascades: These are used for object detection. They are XML files that contain feature descriptors that can be used to detect objects like faces, eyes, cars, etc. Haar cascades are relatively fast but may not be as accurate as some deep learning-based models.\n",
    "\n",
    "2. Deep Learning-based Models: OpenCV also supports deep learning-based models, which are typically more accurate but may require more computational resources. Some popular deep learning frameworks used with OpenCV include:\n",
    "\n",
    "    . YOLO (You Only Look Once): YOLO is a real-time object detection system. It's known for its speed and can detect     multiple objects in an image.\n",
    "\n",
    "    . SSD (Single Shot MultiBox Detector): SSD is another real-time object detection system. It's similar to YOLO but uses a different architecture.\n",
    "\n",
    "    . Faster R-CNN (Region-based Convolutional Neural Networks): Faster R-CNN is a popular deep learning model for object detection. It's known for its accuracy but may be slower compared to YOLO or SSD.\n",
    "\n",
    "    . Mask R-CNN: This is an extension of Faster R-CNN that also predicts object masks in addition to bounding boxes.\n",
    "\n",
    "3. Facial Recognition Models: OpenCV provides pre-trained models specifically for facial recognition tasks. These models can be used for tasks such as face detection, facial landmark detection, and face recognition.\n",
    "\n",
    "4. Scene Understanding Models: OpenCV also offers models for scene understanding tasks such as semantic segmentation, where each pixel in an image is classified into a particular category (e.g., road, sky, person, etc.).\n",
    "\n",
    "5. Text Detection and Recognition Models: OpenCV supports models for detecting and recognizing text in images. These models are useful for tasks such as document scanning, OCR (Optical Character Recognition), and text extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ba03e",
   "metadata": {},
   "source": [
    "# 1.   In this Project we are using :-  Face Detection Using YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1250c0",
   "metadata": {},
   "source": [
    "The ultralytics library provides an easy-to-use interface for working with the YOLO (You Only Look Once) object detection model. YOLO is known for its speed and accuracy in real-time object detection tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592b187",
   "metadata": {},
   "source": [
    "**Ultralytics** is a platform that revolutionizes the world of **vision AI** by empowering people and companies to unleash the positive potential of artificial intelligence. Here are some key points about Ultralytics:\n",
    "\n",
    "1. **Ultralytics YOLOv8**:\n",
    "    - **YOLOv8** is the latest version of the acclaimed **real-time object detection and image segmentation model** developed by Ultralytics.\n",
    "    - It is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of **speed and accuracy**.\n",
    "    - YOLOv8's streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from **edge devices** to **cloud APIs**.\n",
    "    - Whether you are a seasoned machine learning practitioner or new to the field, the **YOLOv8 Docs** provide a comprehensive resource to understand and utilize its features and capabilitiesÂ¹.\n",
    "    - YOLOv8 supports a full range of **vision AI tasks**, including detection, segmentation, pose estimation, tracking, and classification.\n",
    "\n",
    "2. **Brief History of YOLO**:\n",
    "    - **YOLO (You Only Look Once)**, a popular object detection and image segmentation model, was initially developed by Joseph Redmon and Ali Farhadi at the University of Washington.\n",
    "    - YOLO quickly gained popularity for its high speed and accuracy.\n",
    "    - Subsequent versions, such as **YOLOv2**, **YOLOv3**, and **YOLOv4**, introduced various improvements and innovations.\n",
    "    - **YOLOv5** further enhanced performance and added features like hyperparameter optimization and integrated experiment tracking.\n",
    "    - **YOLOv6** (open-sourced by Meituan in 2022) is used in many of the company's autonomous delivery robots.\n",
    "    - **YOLOv7** added additional tasks, including pose estimation on the COCO keypoints dataset.\n",
    "    - **YOLOv8**, developed by Ultralytics, builds on the success of previous versions, introducing new features and improvements for enhanced performance, flexibility, and efficiencyÂ¹.\n",
    "\n",
    "3. **Licensing Options**:\n",
    "    - Ultralytics offers two licensing options:\n",
    "        - **AGPL-3.0 License**: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharingÂ¹.\n",
    "\n",
    "4. **GitHub Repository**:\n",
    "    - Ultralytics maintains an active **GitHub repository** where they offer cutting-edge solutions for a wide range of AI tasks, including detection, segmentation, classification, tracking, and pose estimationÂ².\n",
    "\n",
    "5. **Create and Deploy AI Models**:\n",
    "    - Ultralytics' mission is to empower people and companies to bring their models to life with vision AI tools.\n",
    "    - Their platform allows users to create and deploy powerful AI models from images without coding, using Ultralytics YOLOÂ³.\n",
    "\n",
    "6. **Interactive Notebooks**:\n",
    "    - Ultralytics provides **interactive notebooks** for YOLOv8, covering training, validation, tracking, and more.\n",
    "    - Each notebook is paired with a **YouTube tutorial**, making it easy to learn and implement advanced YOLOv8 featuresâ´.\n",
    "\n",
    "Summary :-  Ultralytics is at the forefront of vision AI, offering state-of-the-art tools and resources for various computer vision tasks. Whether you're a researcher, developer, or enthusiast, Ultralytics aims to maximize the potential of AI in your projects! ðŸš€ðŸ‘ï¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfce7571",
   "metadata": {},
   "source": [
    "# Here's why you might use the YOLO model from Ultralytics :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facbd6d1",
   "metadata": {},
   "source": [
    "Speed: YOLO is known for its speed. It can detect objects in images or videos in real-time, making it suitable for applications where speed is crucial, such as surveillance systems, autonomous vehicles, or interactive applications.\n",
    "\n",
    "Accuracy: Despite its speed, YOLO achieves good accuracy in detecting objects. It uses a single neural network to predict bounding boxes and class probabilities for multiple objects simultaneously, which helps in improving accuracy.\n",
    "\n",
    "Ease of Use: The Ultralytics YOLO implementation provides a user-friendly interface for working with YOLO models. It abstracts away many of the complexities involved in setting up and using deep learning models, making it easier for developers to integrate object detection into their projects.\n",
    "\n",
    "Pre-trained Models: Ultralytics provides pre-trained YOLO models trained on large datasets such as COCO (Common Objects in Context). These pre-trained models can be fine-tuned on specific datasets or used directly for object detection tasks without the need for extensive training.\n",
    "\n",
    "Support for Various Architectures: The Ultralytics YOLO implementation supports different YOLO architectures, including YOLOv5, which is the latest version at the time of writing. These architectures offer various trade-offs between speed and accuracy, allowing you to choose the one that best fits your requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6923a",
   "metadata": {},
   "source": [
    "LINK\n",
    "https://docs.ultralytics.com/                         \n",
    "https://docs.ultralytics.com/tasks/obb/#export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3dcad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries.\n",
    "\n",
    "from ultralytics import YOLO   \n",
    "import cv2                     # library for various computer vision tasks such as image processing, object detection, and video analysis.\n",
    "import torch                   # Accesing PyTorch's functionalities and can use its classes, functions, and methods in your Python script.\n",
    "\n",
    "# PyTorch is a flexible and powerful framework for building and deploying deep learning models.\n",
    "# PyTorch is an open-source machine learning library based on the Torch library.\n",
    "# It is widely used for applications such as computer vision and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74acafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the YOLO model.\n",
    "# Now you can use YOLO class.\n",
    "# \"yolov8n.pt\" is a trained YOLO (You Only Look Once) object detection model.\n",
    "# \"v8n\": The \"v8n\" part in the filename might indicate a specific version or variant of the YOLO model. Differnt model version have different characterstics.\n",
    "# \".pt\" extension: The \".pt\" file extension typically indicates that it's a PyTorch model and dictionary file. denoted by \".pt files\"\n",
    "model = YOLO('yolov8n.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1f75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "# cap = cv2.VideoCapture(0) is used to create a video capture object in OpenCV. \n",
    "# cv2.VideoCapture():- Function provided by the OpenCV library (cv2). It's used to create a video capture object such as video files , camera.\n",
    "# 0 : The argument 0 passed to cv2.VideoCapture() says the camera device to use for capturing video frames. 0 means default camera.\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf04a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 chair, 2 tvs, 545.6ms\n",
      "Speed: 16.7ms preprocess, 545.6ms inference, 17.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 tv, 452.8ms\n",
      "Speed: 10.0ms preprocess, 452.8ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 tv, 213.3ms\n",
      "Speed: 7.2ms preprocess, 213.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 tv, 224.9ms\n",
      "Speed: 5.0ms preprocess, 224.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bottles, 1 tv, 186.6ms\n",
      "Speed: 6.5ms preprocess, 186.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bottles, 1 tv, 202.6ms\n",
      "Speed: 5.5ms preprocess, 202.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 bottles, 1 tv, 1 laptop, 186.5ms\n",
      "Speed: 5.3ms preprocess, 186.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bottles, 1 tv, 209.6ms\n",
      "Speed: 6.9ms preprocess, 209.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 tv, 1 laptop, 213.1ms\n",
      "Speed: 7.0ms preprocess, 213.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# Using while loop creates a continue running loop structure and executes a block of code if condition is true till you dont stop.\n",
    "# This is commonly used for real-time tasks such as video processing or capturing frames from a webcam.\n",
    "while True:\n",
    "    \n",
    "    # Read a frame from the video capture object or webcam.\n",
    "    # ret :  are variable indicates whether the frame was successfully read (returns True if successful).\n",
    "    # cap : Inside the loop, the code reads a frame from a video capture object.\n",
    "    # cap.read() : Is function retrieves(bring back) the next frame from the video stream.\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Check if the frame was successfully read.\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Perform object detection on the frame using the 'model' and display the results.\n",
    "    # After reading a frame, the code performs object detection using a model.\n",
    "    # The results of the object detection are stored in the results variable.\n",
    "    # The show=True argument likely displays the detected objects on the frame.\n",
    "    results = model(frame, show=True)\n",
    "    \n",
    "    # Exiting the loop\n",
    "    # The loop checks for the key press cv2.waitKey function.\n",
    "    # The waitKey() function in OpenCV is used to wait for a keyboard event.\n",
    "    # Check for the 'q' key press with a delay of 1000 milliseconds.\n",
    "    if cv2.waitKey(1000) & 0xFF == ord('q'):           # The ord() function in Python returns the Unicode code point (integer representation) of a given character.\n",
    "        break                                          # break says Exit a loop before normal time.\n",
    "\n",
    "# Release the webcam and close the window.\n",
    "# cap.release() : This line releases the video capture object (cap), which means it frees up any resources (such as camera hardware) that were being used for video capture.\n",
    "# cv2.destroyAllWindows() : This line closes all OpenCV windows that are currently open.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86288625",
   "metadata": {},
   "source": [
    "- 0xFF Why? Because the keyboard input is limited to a certain character set (ASCII), which can be represented using 8 bits (values from 0 to 255).\n",
    "\n",
    "- The purpose of the mask is to ensure that only the least significant 8 bits (i.e., the rightmost byte) of the 32-bit integer are considered.\n",
    "\n",
    "- f the key pressed is â€˜qâ€™, the expression evaluates to True (since 113 & 255 equals 113).\n",
    "\n",
    "- 0xFF == ord('q') ensures that the program responds to the â€˜qâ€™ key specifically, allowing you to control the loop behavior based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f534c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c65a191d",
   "metadata": {},
   "source": [
    "# Face detection can be done by another Method using OpenCV :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce43307",
   "metadata": {},
   "source": [
    "# 2.  In this project we are using  -  Face Detection by Haar Cascades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8bd98",
   "metadata": {},
   "source": [
    "Haar cascade is an algorithm that can detect objects in images, irrespective of their scale in image and location. This algorithm is not so complex and can run in real-time. We can train a haar-cascade detector to detect various objects like cars, bikes, buildings, fruits, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a592b5c",
   "metadata": {},
   "source": [
    "https://pypi.org/project/opencv-python/                 - Link   of   Opencv.\n",
    "\n",
    "\n",
    "Opencv is related to Images and videos.Works as Image processing, image filter can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c79e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2     # imports the OpenCV library into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2575f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking face_cap as variable to face capture.\n",
    "# The haarcascade_frontalface_default.xml file is a pre-trained face detector provided by the developers and maintainers of the OpenCV library. \n",
    "# Used for face detection in images and videos.If apply cascade classifier, it identifies regions in the image that likely contain faces based on certain features (such as edges, corners, and gradients).\n",
    "face_cap = cv2.CascadeClassifier(\"C:/Users/ANIKET/anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Opening camera\n",
    "# Create a variable name as video_cap\n",
    "# cv2.VideoCapture(0) to make camera ON as well as to capture video & (0) will do to Run Time camera enable.\n",
    "video_cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:                                             # While True: Use to Run Infinite Loop till when you don't close.\n",
    "    ret, video_data = video_cap.read()                  # For Images Read we will use 2 variables as ret, video_data.\n",
    "    \n",
    "    # Taking variable col as color for black and white.\n",
    "    # This line converts the video_data (an image or video frame) from the BGR color space to grayscale. Grayscale images contain only intensity information (brightness) without color, making them easier to process for face detection.\n",
    "    col = cv2.cvtColor(video_data, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cap.detectMultiScale(\n",
    "        col,\n",
    "        scaleFactor=1.1,                               # ScaleFactor: How much image size is reduced at each image scale. Smaller value detects smaller faces.\n",
    "        minNeighbors=5,                                # minNeighbhours :How many neighbors each candidate rectangle should have to retain it. Higher values reduce false positives.\n",
    "        minSize=(30, 30),                              # minSize :Sets the minimum object size. Faces smaller than this size are ignored.\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE                  # flag : the algorithm resizes the input image to different scales and then applies the face detection cascade to each scaled version.\n",
    "    )                                                  # flags=cv2.CASCADE_SCALE_IMAGE an additional option or flag used in the detectMultiScale function from the OpenCV library.\n",
    "    for (x, y, w, h) in faces:                         #  extracts the coordinates (x, y) of the top-left corner & the dimensions (w, h) of the bounding rectangle.\n",
    "        cv2.rectangle(video_data, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"video_live\", video_data)                # cv2.imshow function to show image & frame name video_live\n",
    "    \n",
    "    # video_data: The image or video frame where the rectangles will be drawn.\n",
    "    # (x, y): The top-left corner coordinates of the rectangle.\n",
    "    # (x+w, y+h): The bottom-right corner coordinates of the rectangle (calculated from the dimensions).\n",
    "    # (0, 255, 0): The color of the rectangle (in this case, green represented by RGB values).\n",
    "    # 2: The thickness of the rectangle border.\n",
    "    # cv2.imshow(\"video_live\", video_data): shows modified video_data frame with the drawn rectangles. The window title is set to â€œvideo_live.\n",
    "\n",
    "    cv2.putText(video_data, 'Aniket', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        # cv2.putText() function in OpenCV is used to draw text on an image. \n",
    "        # frame       : This is the image (or frame) on which you want to draw the text.\n",
    "        # (x, y - 10) : The coordinates of the >>bottom-left corner << of the text string in the image. The `x` and `y` values determine the position where the text will appear. By subtracting 10 from the `y` coordinate, the text will be slightly above the specified point.\n",
    "        # cv2.FONT_HERSHEY_SIMPLEX : The font type to use for the text. In this case, it's the simple font style.\n",
    "        # 0.9                      : The font scale factor. It determines the size of the text relative to the original font size.\n",
    "        # (36, 255, 12)            : The color of the text. Here, it's an RGB color represented by `(B, G, R)` values (blue, green, red).\n",
    "        # 2                        : The thickness of the text line.\n",
    "        # video_data: This is the image or video frame on which you want to display the text.\n",
    "        \n",
    "    # Show the video frame\n",
    "    cv2.imshow(\"video_live\", video_data)\n",
    "\n",
    "    # Now to close the camera because While True\n",
    "    if cv2.waitKey(10) == ord(\"a\"):                     # .waitKey(10) is used to stop Images or camera for a particular time.\n",
    "        break\n",
    "\n",
    "video_cap.release()                                     # .release function used to release video capture.\n",
    "cv2.destroyAllWindows()                                 # Close all windows after exiting the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855abccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR ENABLE CAMERA.\n",
    "# # 1st opening camera\n",
    "# # Create a variable name as video_cap\n",
    "# # cv2.Videocapture(0) to make camera ON as wll as to capture video & (0) will do to Run Time camera enable .\n",
    "# video_cap = cv2.VideoCapture(0)\n",
    "# while True :                                         # While True: Use to Run Infinite Loop till when you dont close.\n",
    "# # For image Read we will use 2 variables as ret , video_data\n",
    "# ret , video_data = video_cap.read()\n",
    "# cv2.imshow(\"video_live\",video_data)              # cv2.imshow function to show image & frame name video_live\n",
    "# # Now to close the camera because While True \n",
    "# if cv2.waitkey(10) == ord(\"a\"):                  # .waitkey(10) is used to stop Images or camera for a particular time.\n",
    "#     break\n",
    "# video_cap.release()                                  # .relaese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908734bd",
   "metadata": {},
   "source": [
    "Conclusion : Real-time face detection system effectively detects human faces in live video streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b5eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
